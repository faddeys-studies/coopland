training:
#  entropy_strength: 0.01
  sync_each_n_games: 20
  learning_rate: 0.001
  actor_loss_weight: 0.1
  critic_loss_weight: 1.0
  logit_regularization_strength: 0.01
  optimizer_kwargs:
    decay: 0.995
#  max_game_steps: 25
model:
  rnn_units: [47, 53]
  max_agents: 8
  comm:
    type: comm_net
    units: [23]
    can_see_others: true

maze_size: 5

reward:
  discount_rate: 0.95

  step_reward: 0.5
  exit_reward: 2.0
  one_agent_exit_reward: 0.0

  exit_reward_diff_decay: 0.0
  average_over_team: false
