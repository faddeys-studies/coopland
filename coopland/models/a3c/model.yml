training:
  discount_rate: 0.9
#  entropy_strength: 0.0003
  sync_each_n_games: 1
  learning_rate: 0.001
  actor_loss_weight: 1.0
  critic_loss_weight: 0.25
  use_data_augmentation: false
  logit_regularization_strength: 0.0003
  actor_label_smoothing: 0.01
model:
  rnn_units: [50, 50]
  max_agents: 1
  use_visible_agents: false
  use_communication: false
  comm_units: []

maze_size: 5
